# PHÃ‚N TÃCH CHI TIáº¾T CÃC LUá»’NG HOáº T Äá»˜NG

## ğŸ“‹ Má»¤C Lá»¤C
1. [Luá»“ng Extension Scanning](#luá»“ng-extension-scanning)
2. [Luá»“ng Batch Processing](#luá»“ng-batch-processing)
3. [Luá»“ng Authentication](#luá»“ng-authentication)
4. [Luá»“ng ML Prediction](#luá»“ng-ml-prediction)
5. [Luá»“ng User Feedback](#luá»“ng-user-feedback)
6. [Luá»“ng Admin Dashboard](#luá»“ng-admin-dashboard)

---

## ğŸ” 1. LUá»’NG EXTENSION SCANNING

### Má»¥c Ä‘Ã­ch
Tá»± Ä‘á»™ng quÃ©t vÃ  phÃ¢n loáº¡i comments trÃªn cÃ¡c trang máº¡ng xÃ£ há»™i khi ngÆ°á»i dÃ¹ng browse.

### Chi tiáº¿t tá»«ng bÆ°á»›c

#### BÆ°á»›c 1: Platform Detection (content.js)
```javascript
// XÃ¡c Ä‘á»‹nh platform hiá»‡n táº¡i
let currentPlatform = '';
if (window.location.hostname.includes("facebook.com")) {
  currentPlatform = PLATFORMS.FACEBOOK;
} else if (window.location.hostname.includes("youtube.com")) {
  currentPlatform = PLATFORMS.YOUTUBE;
} else if (window.location.hostname.includes("twitter.com")) {
  currentPlatform = PLATFORMS.TWITTER;
}
```

**Output**: `currentPlatform` variable Ä‘Æ°á»£c set

---

#### BÆ°á»›c 2: Start MutationObserver
```javascript
commentObserver = new MutationObserver((mutations) => {
  const commentSelectors = getCommentSelectors();
  
  for (const mutation of mutations) {
    if (mutation.type === 'childList' && mutation.addedNodes.length > 0) {
      for (const node of mutation.addedNodes) {
        if (node.nodeType === Node.ELEMENT_NODE) {
          const comments = node.matches(commentSelectors) ? 
            [node] : Array.from(node.querySelectorAll(commentSelectors));
          
          for (const comment of comments) {
            processComment(comment);
          }
        }
      }
    }
  }
});
```

**Comment Selectors** cho tá»«ng platform:
- **Facebook**: `.x1y1aw1k div[dir="auto"]`
- **YouTube**: `ytd-comment-renderer #content-text`
- **Twitter**: `[data-testid="tweetText"]`

---

#### BÆ°á»›c 3: Process Comment
```javascript
function processComment(commentElement) {
  const commentId = getCommentId(commentElement);
  
  // Skip if already processed
  if (processedComments.has(commentId)) return;
  processedComments.add(commentId);
  
  // Get comment text
  const commentText = getCommentText(commentElement);
  if (!commentText || commentText.trim().length < 5) return;
  
  // Send to background for analysis
  chrome.runtime.sendMessage({
    action: "analyzeText",
    text: commentText,
    platform: currentPlatform,
    commentId: commentId
  }, (response) => {
    if (response && !response.error) {
      applyToxicityIndicator(commentElement, response);
    }
  });
}
```

**Key Points**:
- Comments Ä‘Æ°á»£c track báº±ng `processedComments` Set Ä‘á»ƒ trÃ¡nh process duplicate
- Minimum length: 5 characters
- Async communication vá»›i background script

---

#### BÆ°á»›c 4: Buffer Comments (background.js)

```javascript
let commentsBuffer = [];
const BATCH_SIZE = 100;
let batchProcessingTimeout = null;

function addToBuffer(text, platform, commentId, sourceUrl) {
  return new Promise((resolve, reject) => {
    const commentIdentifier = `${platform}_${commentId}`;
    
    // Check duplicate
    if (commentsBuffer.findIndex(item => 
      item.identifier === commentIdentifier) >= 0) {
      resolve({ status: "buffered" });
      return;
    }
    
    // Add to buffer
    commentsBuffer.push({
      text, platform, 
      platform_id: commentId,
      source_url: sourceUrl,
      identifier: commentIdentifier,
      resolve, reject
    });
    
    // Process if batch is full
    if (commentsBuffer.length >= BATCH_SIZE) {
      clearTimeout(batchProcessingTimeout);
      processCommentsBatch();
    } else if (!batchProcessingTimeout) {
      // Set timeout náº¿u chÆ°a Ä‘á»§ batch
      batchProcessingTimeout = setTimeout(() => {
        processCommentsBatch();
        batchProcessingTimeout = null;
      }, 2000); // 2 seconds
    }
  });
}
```

**Batch Strategy**:
- **BATCH_SIZE**: 100 comments
- **Timeout**: 2 giÃ¢y náº¿u khÃ´ng Ä‘á»§ batch
- **Rationale**: Giáº£m sá»‘ lÆ°á»£ng API calls, tÄƒng performance

---

#### BÆ°á»›c 5: Send Batch Request
```javascript
async function processCommentsBatch() {
  const batchToProcess = [...commentsBuffer];
  commentsBuffer = []; // Reset buffer
  
  const batchItems = batchToProcess.map(item => ({
    text: item.text,
    platform: item.platform,
    platform_id: item.platform_id,
    source_url: item.source_url,
    metadata: item.metadata
  }));
  
  // Call API
  const response = await fetch(`${API_ENDPOINT}/extension/batch-detect`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": `Basic ${BASIC_AUTH_TOKEN}`
    },
    body: JSON.stringify({
      items: batchItems,
      store_clean: false,
      save_to_db: false
    })
  });
  
  const batchResult = await response.json();
  
  // Map results vÃ  resolve promises
  batchToProcess.forEach(item => {
    const result = resultsMap[item.identifier];
    if (result) {
      item.resolve(result);
    }
  });
}
```

---

#### BÆ°á»›c 6: Backend Processing (backend/api/routes/extension.py)

```python
@router.post("/batch-detect", response_model=BatchPredictionResponse)
async def extension_batch_detect(
    request: BatchPredictionRequest,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_optional_current_user)
):
    results = []
    
    for item in request.items:
        # Predict
        prediction, confidence, probabilities = ml_model.predict(
            item['text'], 
            model_type=request.model_type
        )
        
        prediction_text = {
            0: "bÃ¬nh thÆ°á»ng", 
            1: "xÃºc pháº¡m", 
            2: "thÃ¹ ghÃ©t", 
            3: "spam"
        }[prediction]
        
        # Save to DB if needed
        if request.save_to_db and (prediction != 0 or request.store_clean):
            background_tasks.add_task(
                store_extension_prediction,
                db=db,
                content=item['text'],
                platform=item.get('platform'),
                prediction=prediction,
                confidence=confidence,
                user_id=current_user.id if current_user else None
            )
        
        results.append({
            "text": item['text'],
            "prediction": prediction,
            "confidence": confidence,
            "probabilities": probabilities,
            "prediction_text": prediction_text
        })
    
    return {
        "results": results,
        "count": len(results),
        "timestamp": datetime.utcnow().isoformat()
    }
```

---

#### BÆ°á»›c 7: Apply Visual Indicators (content.js)

```javascript
function applyToxicityIndicator(commentElement, prediction) {
  const categoryNames = ["clean", "offensive", "hate", "spam"];
  const category = categoryNames[prediction.prediction];
  
  const style = CATEGORY_STYLES[category];
  
  // Create indicator
  const indicator = document.createElement('div');
  indicator.className = `toxic-indicator ${style.className}`;
  indicator.style.backgroundColor = style.color;
  indicator.textContent = style.label;
  indicator.title = `PhÃ¢n loáº¡i: ${style.label} (${(prediction.confidence * 100).toFixed(1)}%)`;
  
  // Create report button
  const reportBtn = document.createElement('button');
  reportBtn.className = 'report-incorrect-btn';
  reportBtn.textContent = 'BÃ¡o cÃ¡o phÃ¢n tÃ­ch sai';
  reportBtn.addEventListener('click', (e) => {
    reportIncorrectAnalysis(commentElement, category);
  });
  
  // Add to DOM
  const indicatorContainer = document.createElement('div');
  indicatorContainer.className = 'toxic-indicator-container';
  indicatorContainer.appendChild(indicator);
  indicatorContainer.appendChild(reportBtn);
  
  commentElement.parentNode.insertBefore(
    indicatorContainer, 
    commentElement.nextSibling
  );
  
  // Add border
  commentElement.style.borderLeft = `3px solid ${style.color}`;
  commentElement.style.paddingLeft = '10px';
  
  // Blur hate speech
  if (category === 'hate') {
    commentElement.classList.add('toxic-blur');
    const revealBtn = document.createElement('button');
    revealBtn.textContent = 'Hiá»‡n ná»™i dung';
    revealBtn.onclick = () => {
      commentElement.classList.remove('toxic-blur');
      revealBtn.remove();
    };
    indicatorContainer.appendChild(revealBtn);
  }
}
```

**Visual Indicators**:
- **Border color** matching category
- **Label badge** vá»›i tÃªn category
- **Confidence** hiá»ƒn thá»‹ trong tooltip
- **Report button** Ä‘á»ƒ feedback
- **Blur effect** cho hate speech vá»›i reveal button

---

### Timeline Example

```
T+0ms:    User navigates to facebook.com/post/123
T+100ms:  MutationObserver detects new comment nodes
T+150ms:  processComment() called for each comment
T+200ms:  10 comments added to buffer
T+2000ms: Timeout triggers â†’ processCommentsBatch()
T+2050ms: API request sent vá»›i 10 comments
T+2500ms: Backend processes batch
          - Preprocess 10 texts
          - ML model prediction
          - Return results
T+2550ms: Results mapped vÃ  resolve promises
T+2600ms: Visual indicators applied to 10 comments
```

---

## ğŸ“¦ 2. LUá»’NG BATCH PROCESSING

### Use Case
User muá»‘n phÃ¢n tÃ­ch má»™t lÆ°á»£ng lá»›n text cÃ¹ng lÃºc (tá»« file hoáº·c paste).

### Chi tiáº¿t Implementation

#### Frontend (popup.js)

```javascript
async function processBatchText() {
  const batchInputText = batchInput.value.trim();
  
  // Split by newlines
  const allComments = batchInputText.split('\n')
    .map(text => text.trim())
    .filter(text => text.length > 0);
  
  if (allComments.length === 0) {
    showNotification('KhÃ´ng tÃ¬m tháº¥y bÃ¬nh luáº­n há»£p lá»‡', 'warning');
    return;
  }
  
  // Get auth data
  const authData = await getAuthData();
  const isLoggedIn = !!authData;
  
  // Prepare headers
  const headers = {
    'Content-Type': 'application/json',
  };
  if (isLoggedIn) {
    headers['Authorization'] = `Bearer ${authData.access_token}`;
  }
  
  // Process in chunks
  const CHUNK_SIZE = 100;
  const totalChunks = Math.ceil(allComments.length / CHUNK_SIZE);
  let allResults = [];
  
  for (let chunkIndex = 0; chunkIndex < totalChunks; chunkIndex++) {
    // Update progress
    const progressPercent = Math.round((chunkIndex / totalChunks) * 100);
    updateProgress(progressPercent);
    
    // Get chunk
    const startIndex = chunkIndex * CHUNK_SIZE;
    const endIndex = Math.min(startIndex + CHUNK_SIZE, allComments.length);
    const currentChunk = allComments.slice(startIndex, endIndex);
    
    // Prepare items
    const items = currentChunk.map(text => ({
      text: text,
      platform: platform
    }));
    
    // API call
    const response = await fetch(`${API_ENDPOINT}/extension/batch-detect`, {
      method: 'POST',
      headers: headers,
      body: JSON.stringify({
        items: items,
        save_to_db: isLoggedIn,
        store_clean: false,
        model_type: modelType
      })
    });
    
    const result = await response.json();
    allResults = allResults.concat(result.results);
    
    // Delay between chunks
    if (chunkIndex < totalChunks - 1) {
      await new Promise(resolve => setTimeout(resolve, 300));
    }
  }
  
  // Display results
  displayBatchResults({ results: allResults, count: allResults.length });
}
```

**Key Features**:
- **Chunking**: Process 100 items per API call
- **Progress tracking**: Real-time progress bar
- **Delay between chunks**: 300ms Ä‘á»ƒ trÃ¡nh overwhelm server
- **Auto-save**: Chá»‰ save náº¿u logged in

---

#### Display Results

```javascript
function displayBatchResults(batchData) {
  batchResultList.innerHTML = '';
  
  const results = batchData.results || [];
  const counts = { clean: 0, offensive: 0, hate: 0, spam: 0 };
  
  results.forEach(result => {
    // Update counts
    counts[categoryNames[result.prediction]]++;
    
    // Create result item
    const resultItem = document.createElement('div');
    resultItem.className = 'batch-result-item';
    resultItem.innerHTML = `
      <div class="result-text">${result.text}</div>
      <div class="result-meta">
        <span class="label ${categoryNames[result.prediction]}">
          ${result.prediction_text}
        </span>
        <span class="confidence">${Math.round(result.confidence * 100)}%</span>
      </div>
    `;
    batchResultList.appendChild(resultItem);
  });
  
  // Update summary
  document.getElementById('batch-clean-count').textContent = counts.clean;
  document.getElementById('batch-offensive-count').textContent = counts.offensive;
  document.getElementById('batch-hate-count').textContent = counts.hate;
  document.getElementById('batch-spam-count').textContent = counts.spam;
}
```

---

## ğŸ” 3. LUá»’NG AUTHENTICATION

### Register Flow

```
User Input (popup)
    â”‚
    â–¼
Validate Fields
    â”œâ”€ Username: required, unique
    â”œâ”€ Email: required, unique, valid format
    â”œâ”€ Password: required
    â””â”€ Confirm Password: must match
    â”‚
    â–¼
POST /auth/register
    {
      "username": "string",
      "email": "string",
      "name": "string",
      "password": "string"
    }
    â”‚
    â–¼
Backend Processing
    â”œâ”€ Check username exists
    â”œâ”€ Check email exists
    â”œâ”€ Get default "user" role
    â”œâ”€ Hash password (bcrypt)
    â”œâ”€ Create User record
    â””â”€ Create Log entry
    â”‚
    â–¼
Response: 201 Created
    {
      "id": 123,
      "username": "string",
      "email": "string",
      "role": "user",
      ...
    }
    â”‚
    â–¼
Frontend Actions
    â”œâ”€ Show success notification
    â”œâ”€ Switch to login form
    â””â”€ Pre-fill username
```

---

### Login Flow

```
User Input (popup)
    â”‚
    â–¼
POST /auth/token (Form Data)
    username=...&password=...
    â”‚
    â–¼
Backend Processing
    â”œâ”€ Query User by username
    â”œâ”€ Verify password (bcrypt.verify)
    â”œâ”€ Check is_active
    â”œâ”€ Generate JWT token
    â”‚   â”œâ”€ Payload: { sub: username, role: role_name }
    â”‚   â”œâ”€ Secret: SECRET_KEY
    â”‚   â”œâ”€ Algorithm: HS256
    â”‚   â””â”€ Expiry: ACCESS_TOKEN_EXPIRE_MINUTES
    â”œâ”€ Update last_login, last_login_ip
    â””â”€ Create Log entry
    â”‚
    â–¼
Response: 200 OK
    {
      "access_token": "eyJ...",
      "token_type": "bearer",
      "user_id": 123,
      "username": "string",
      "role": "user",
      "expires_in": 86400
    }
    â”‚
    â–¼
Frontend Actions
    â”œâ”€ Save to chrome.storage.local
    â”‚   {
    â”‚     "toxicDetector_auth": {
    â”‚       "access_token": "...",
    â”‚       "user_id": 123,
    â”‚       ...
    â”‚     }
    â”‚   }
    â”œâ”€ Fetch /auth/me to get full profile
    â”œâ”€ Update body class: add 'logged-in'
    â”œâ”€ Show profile section
    â”œâ”€ Update tab visibility
    â”‚   â”œâ”€ Show: Stats tab
    â”‚   â”œâ”€ Show: Batch mode
    â”‚   â””â”€ Enable: Save data checkbox
    â””â”€ Show success notification
```

---

### Authenticated Request Flow

```
Every API Request (if logged in)
    â”‚
    â–¼
Get Token from Storage
    chrome.storage.local.get(['toxicDetector_auth'])
    â”‚
    â–¼
Add Authorization Header
    headers: {
      'Authorization': `Bearer ${authData.access_token}`
    }
    â”‚
    â–¼
Backend Middleware (dependencies.py)
    â”‚
    â”œâ”€ Extract token from header
    â”œâ”€ Decode JWT token
    â”‚   â”œâ”€ Verify signature with SECRET_KEY
    â”‚   â”œâ”€ Check expiration
    â”‚   â””â”€ Extract username from 'sub'
    â”œâ”€ Query User by username
    â”œâ”€ Check is_active
    â”œâ”€ Update last_activity
    â””â”€ Return User object
    â”‚
    â–¼
If Valid: Process Request
If Invalid: Return 401 Unauthorized
    â”‚
    â–¼
Frontend Error Handler (401)
    â”œâ”€ Clear auth data
    â”œâ”€ Update UI to logged-out state
    â”œâ”€ Show login form
    â””â”€ Show notification: "Vui lÃ²ng Ä‘Äƒng nháº­p láº¡i"
```

---

## ğŸ¤– 4. LUá»’NG ML PREDICTION

### Text Preprocessing Pipeline

```python
def preprocess_text(text: str) -> str:
    """
    Step-by-step preprocessing
    """
    # 1. Lowercase
    text = text.lower()
    
    # 2. Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    # 3. Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # 4. Remove/normalize punctuation
    text = re.sub(r'[.,;:!?()"\'\[\]/\\]', ' ', text)
    
    # 5. Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 6. Vietnamese tokenization
    try:
        from underthesea import word_tokenize
        text = word_tokenize(text, format="text")
    except ImportError:
        pass  # Skip if underthesea not available
    
    return text
```

---

### Model Prediction Flow

```python
def predict(self, text: str, model_type: str = None):
    """
    Full prediction pipeline
    """
    # 1. Load model náº¿u chÆ°a load
    if not self.loaded:
        self.load_model()
    
    # 2. Handle empty text
    if not text or not text.strip():
        return 0, 1.0, {"clean": 1.0, "offensive": 0, "hate": 0, "spam": 0}
    
    # 3. Switch model náº¿u cáº§n
    if model_type and model_type != self.model_type:
        model_files = {
            "lstm": "model/best_model_LSTM.h5",
            "cnn": "model/cnn/model.safetensors",
            "bert": "model/bert/model.safetensors",
            "phobert": "model/phobert/model.safetensors",
            ...
        }
        temp_model = ModelAdapter.load_model(model_files[model_type])
        # Use temp_model for this prediction
    
    # 4. Preprocess
    processed_input = self.preprocess(text)
    
    # 5. Model inference
    prediction = self.model.predict(processed_input)
    probs = prediction[0]
    
    # 6. Get class vÃ  confidence
    predicted_class = np.argmax(probs)
    confidence = float(probs[predicted_class])
    
    # 7. Apply spam heuristics
    _, spam_features = preprocess_for_spam_detection(text)
    
    spam_score = 0
    if spam_features.get('has_url'):
        spam_score += 0.2
    if spam_features.get('has_suspicious_url'):
        spam_score += 0.3
    if spam_features.get('url_count', 0) > 1:
        spam_score += 0.1 * spam_features['url_count']
    if spam_features.get('spam_keyword_count', 0) > 0:
        spam_score += 0.15 * spam_features['spam_keyword_count']
    if spam_features.get('has_excessive_punctuation'):
        spam_score += 0.1
    if spam_features.get('has_all_caps_words'):
        spam_score += 0.1
    
    # 8. Override náº¿u spam_score cao
    spam_class = 3
    if predicted_class != spam_class and confidence < 0.8 and spam_score > 0.5:
        predicted_class = spam_class
        confidence = max(confidence, spam_score)
        probabilities = {label: 0.1 for label in self.labels}
        probabilities[self.labels[spam_class]] = spam_score
    else:
        probabilities = {
            self.labels[i]: float(probs[i]) 
            for i in range(len(self.labels))
        }
    
    # 9. Return
    return int(predicted_class), confidence, probabilities
```

**Key Points**:
- **Model switching**: CÃ³ thá»ƒ chá»n model khÃ¡c nhau cho má»—i prediction
- **Spam heuristics**: Rule-based system bá»• sung cho ML
- **Confidence threshold**: 0.8 Ä‘á»ƒ apply spam rules
- **Probabilities**: Return Ä‘áº§y Ä‘á»§ xÃ¡c suáº¥t cho táº¥t cáº£ classes

---

### Spam Detection Heuristics Details

```python
def preprocess_for_spam_detection(text: str):
    """
    Extract spam-related features
    """
    features = {}
    
    # URL detection
    url_pattern = r'https?://\S+|www\.\S+'
    urls = re.findall(url_pattern, text)
    features['has_url'] = len(urls) > 0
    features['url_count'] = len(urls)
    
    # Suspicious URL domains
    suspicious_domains = [
        'bit.ly', 'tinyurl.com', 'goo.gl',
        't.co', 'ow.ly', 'is.gd'
    ]
    features['has_suspicious_url'] = any(
        domain in url for url in urls for domain in suspicious_domains
    )
    
    # Spam keywords (Vietnamese)
    spam_keywords = [
        'giáº£m giÃ¡', 'khuyáº¿n mÃ£i', 'mua ngay', 'click here',
        'inbox', 'zalo', 'liÃªn há»‡', 'táº¡i Ä‘Ã¢y', 'free'
    ]
    text_lower = text.lower()
    features['spam_keyword_count'] = sum(
        1 for keyword in spam_keywords if keyword in text_lower
    )
    
    # Excessive punctuation (>30% of text)
    punctuation_count = len([c for c in text if c in '!?.,;:'])
    features['has_excessive_punctuation'] = (
        punctuation_count / max(len(text), 1) > 0.3
    )
    
    # All caps words
    words = text.split()
    caps_words = [w for w in words if w.isupper() and len(w) > 3]
    features['has_all_caps_words'] = len(caps_words) > 2
    
    return text, features
```

---

## ğŸ“¢ 5. LUá»’NG USER FEEDBACK

### Report Incorrect Analysis

```
User clicks "BÃ¡o cÃ¡o phÃ¢n tÃ­ch sai"
    â”‚
    â–¼
content.js: reportIncorrectAnalysis()
    â”œâ”€ Get comment text
    â”œâ”€ Get predicted category
    â””â”€ Get comment ID
    â”‚
    â–¼
Send to background.js
    chrome.runtime.sendMessage({
      action: "reportIncorrectAnalysis",
      text: commentText,
      predictedCategory: category,
      commentId: commentId
    })
    â”‚
    â–¼
background.js: reportIncorrectAnalysis()
    â”‚
    â–¼
POST /extension/report
    {
      "text": "string",
      "predicted_category": "offensive",
      "comment_id": "string",
      "source_url": "string",
      "metadata": {
        "source": "extension",
        "browser": "user_agent",
        "timestamp": "ISO8601",
        "version": "1.0.0"
      }
    }
    â”‚
    â–¼
Backend: Store Report
    â”œâ”€ Create Log entry
    â”œâ”€ (Optional) Create Feedback entry
    â””â”€ (Future) Add to retraining dataset
    â”‚
    â–¼
Response: 200 OK
    {"detail": "BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c ghi nháº­n"}
    â”‚
    â–¼
Frontend Actions
    â”œâ”€ Show success notification
    â”œâ”€ Update button state: "ÄÃ£ bÃ¡o cÃ¡o"
    â””â”€ Disable button
```

---

## ğŸ“Š 6. LUá»’NG ADMIN DASHBOARD

### Dashboard Data Flow

```
Admin navigates to Dashboard
    â”‚
    â–¼
GET /admin/dashboard?period=month
    Headers: Authorization: Bearer <admin_token>
    â”‚
    â–¼
Backend: get_dashboard_data()
    â”‚
    â”œâ”€ Verify admin role
    â”œâ”€ Calculate date range (day/week/month/year)
    â”œâ”€ Query statistics
    â”‚   â”œâ”€ Total comments
    â”‚   â”œâ”€ Comments by category (clean/offensive/hate/spam)
    â”‚   â”œâ”€ Total users
    â”‚   â”œâ”€ Active users in period
    â”‚   â””â”€ Comments by platform
    â”œâ”€ Get ML model stats
    â””â”€ Return aggregated data
    â”‚
    â–¼
Response: 200 OK
    {
      "statistics": {
        "total_comments": 1234,
        "clean_comments": 800,
        "offensive_comments": 200,
        "hate_comments": 134,
        "spam_comments": 100,
        "total_users": 50,
        "active_users": 30
      },
      "platforms": {
        "facebook": 600,
        "youtube": 400,
        "twitter": 234
      },
      "model_stats": {
        "type": "lstm",
        "accuracy": 0.92,
        ...
      },
      "period": "month"
    }
    â”‚
    â–¼
Dashboard Rendering
    â”œâ”€ Update statistics cards
    â”œâ”€ Render pie chart (by category)
    â”œâ”€ Render bar chart (by platform)
    â””â”€ Display recent comments table
```

---

### Comment Management Flow

```
Admin searches comments
    â”‚
    â–¼
GET /admin/comments?
    platform=facebook&
    prediction=2&
    start_date=2024-01-01&
    end_date=2024-12-31&
    search=keyword&
    skip=0&
    limit=100
    â”‚
    â–¼
Backend: get_comments()
    â”‚
    â”œâ”€ Verify admin role
    â”œâ”€ Build query vá»›i filters
    â”‚   â”œâ”€ Platform filter
    â”‚   â”œâ”€ Prediction filter
    â”‚   â”œâ”€ Date range filter
    â”‚   â”œâ”€ Search in content
    â”‚   â”œâ”€ Confidence range
    â”‚   â””â”€ User filter
    â”œâ”€ Execute query vá»›i pagination
    â””â”€ Return comments list
    â”‚
    â–¼
Response: 200 OK
    [
      {
        "id": 123,
        "content": "...",
        "prediction": 2,
        "prediction_text": "hate",
        "confidence": 0.85,
        "platform": "facebook",
        "source_user_name": "user123",
        "created_at": "2024-10-19T..."
      },
      ...
    ]
    â”‚
    â–¼
Dashboard Actions
    â”œâ”€ Display in table
    â”œâ”€ Enable actions:
    â”‚   â”œâ”€ View details
    â”‚   â”œâ”€ Edit/Review
    â”‚   â”œâ”€ Delete
    â”‚   â””â”€ Export
    â””â”€ Show pagination controls
```

---

### Export Comments Flow

```
Admin clicks "Export"
    â”‚
    â–¼
Select format: CSV | Excel | PDF
    â”‚
    â–¼
GET /admin/export/comments?
    format=csv&
    (same filters as search)
    â”‚
    â–¼
Backend: export_comments()
    â”‚
    â”œâ”€ Verify admin role
    â”œâ”€ Query comments vá»›i filters (no limit)
    â”œâ”€ Convert to DataFrame
    â”œâ”€ Generate file theo format
    â”‚   â”œâ”€ CSV: df.to_csv()
    â”‚   â”œâ”€ Excel: df.to_excel()
    â”‚   â””â”€ PDF: ReportLab + Table
    â””â”€ Create Log entry
    â”‚
    â–¼
Response: File Download
    Content-Type: text/csv | application/vnd.ms-excel | application/pdf
    Content-Disposition: attachment; filename=comments_export.xxx
    â”‚
    â–¼
Browser downloads file
```

---

## ğŸ”„ Tá»”NG Káº¾T INTERACTION PATTERNS

### 1. Extension â†” Backend
- **Protocol**: HTTP REST API
- **Auth**: Basic Auth hoáº·c JWT Bearer token
- **Data Format**: JSON
- **Batch Size**: 100 items max per request
- **Retry Logic**: Fallback to individual analysis on batch failure

### 2. Popup â†” Background
- **Protocol**: Chrome Runtime Messaging
- **Method**: `chrome.runtime.sendMessage()`
- **Pattern**: Request-Response vá»›i callback
- **Error Handling**: Error object trong response

### 3. Content â†” Background
- **Protocol**: Chrome Runtime Messaging
- **Pattern**: 
  - Content â†’ Background: Request analysis
  - Background â†’ Content: Return result
- **Async**: Promise-based communication

### 4. Frontend â†” Backend (Dashboard)
- **Protocol**: HTTP REST API
- **Auth**: JWT Bearer token
- **CORS**: Enabled vá»›i specific origins
- **Rate Limiting**: 100 requests / 60 seconds

---

## ğŸ“ˆ PERFORMANCE CONSIDERATIONS

### Extension
- **Debouncing**: MutationObserver events
- **Batching**: 100 comments per API call
- **Timeout**: 2s before processing incomplete batch
- **Caching**: Processed comments in Set
- **Lazy Loading**: Only analyze visible comments

### Backend
- **Connection Pooling**: SQLAlchemy pool_size=5
- **Model Caching**: Singleton pattern cho MLModel
- **Background Tasks**: Database writes
- **Rate Limiting**: IP-based with Redis (future)
- **Vector Indexing**: For similarity search

### Database
- **Indexes**: 
  - users(username, email)
  - comments(platform, prediction, created_at)
  - logs(timestamp, user_id)
- **Partitioning**: By created_at (future)
- **Archiving**: Old comments (future)

---

*TÃ i liá»‡u Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng bá»Ÿi AI Assistant*
*NgÃ y táº¡o: 2025-10-19*

